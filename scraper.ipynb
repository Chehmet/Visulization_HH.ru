{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4647870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chulpan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Scraping jobs:  79%|███████▊  | 787/1000 [33:20<09:01,  2.54s/job]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraped 787 job postings successfully.\n",
      "Dashboard data saved to data.json\n",
      "\n",
      "Summary of processed data:\n",
      "  Top skills found: 20\n",
      "  Top terms found: 20\n",
      "  Experience level categories: 4\n",
      "  Salary type categories: 5\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "BASE_URL_HH = \"https://hh.ru\"\n",
    "TARGET_JOB_SEARCH_QUERY = \"Python разработчик\"\n",
    "TARGET_AREA_ID = \"113\"\n",
    "NUM_JOBS_TO_SCRAPE = 1000\n",
    "MAX_SCRAPE_LIMIT = 1050\n",
    "MAX_PAGES_TO_CHECK = 100\n",
    "\n",
    "SKILL_KEYWORDS_RU = [\n",
    "    'python', 'java', 'c++', 'c#', 'javascript', 'sql', 'nosql', 'mongodb', 'react', 'angular', 'vue', 'node.js',\n",
    "    'django', 'flask', 'spring', 'api', 'rest', 'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'git', 'linux',\n",
    "    'machine learning', 'deep learning', 'data analysis', 'data science', 'pandas', 'numpy', 'scikit-learn',\n",
    "    'tensorflow', 'pytorch', 'nlp', 'computer vision', 'big data', 'hadoop', 'spark', 'kafka', 'rabbitmq',\n",
    "    'html', 'css', 'typescript', 'php', 'laravel', 'symfony', 'ruby', 'swift', 'kotlin', 'ios', 'android',\n",
    "    'sql', 'postgresql', 'mysql', 'oracle', 'ms sql', 'redis', 'memcached', 'clickhouse',\n",
    "    'agile', 'scrum', 'jira', 'английский', 'коммуникабельность', 'ответственность', 'команда', 'аналитический',\n",
    "    'системное мышление', 'разработка', 'тестирование', 'автоматизация', 'алгоритмы', 'структуры данных',\n",
    "    'микросервисы', 'devops', 'ci/cd', 'сети', 'системный администратор', 'базы данных', 'проектирование',\n",
    "    'управление проектами', 'product management', 'ux', 'ui', 'дизайн', 'аналитика', 'отчетность', 'bi',\n",
    "    '1с', 'битрикс', 'ооп', 'asyncio', 'fastapi', 'celery', 'airflow', 'etl', 'data warehouse', 'dwh',\n",
    "    'высшее образование', 'техническое образование', 'математика', 'статистика', 'linux', 'bash', 'unix',\n",
    "    'go', 'golang', 'scala', 'rust', 'perl', 'delphi', 'objective-c', 'swiftui', 'kotlin multiplatform',\n",
    "    'kubernetes', 'openshift', 'terraform', 'ansible', 'jenkins', 'gitlab ci', 'teamcity', 'prometheus', 'grafana',\n",
    "    'zabbix', 'elk', 'http', 'tcp/ip', 'dns', 'vpn', 'сетевое администрирование', 'информационная безопасность',\n",
    "    'рефакторинг', 'архитектура по', 'опыт работы', 'управление командой'\n",
    "]\n",
    "\n",
    "STOP_WORDS_RU = set(stopwords.words('russian')) | set(stopwords.words('english'))\n",
    "COMMON_NOISE_RU = {\n",
    "    'работа', 'вакансия', 'компания', 'требования', 'обязанности', 'условия', 'опыт', 'разработка',\n",
    "    'сотрудник', 'проект', 'задачи', 'навыки', 'уровень', 'москва', 'россия', 'ищем', 'специалист',\n",
    "    'команда', 'развитие', 'заработная', 'плата', 'оформление', 'тк', 'рф', 'график', 'полный', 'день',\n",
    "    'офис', 'возможность', 'рост', 'дружный', 'коллектив', 'приглашаем', 'присоединиться', 'нашей',\n",
    "    'нашу', 'вас', 'мы', 'год', 'лет', 'также', 'наличие', 'знание', 'умение', 'работы', 'задача',\n",
    "    'понимание', 'принципы', 'процесс', 'желание', 'участие', 'поддержка', 'обеспечение', 'создание',\n",
    "    'анализ', 'контроль', 'оптимизация', 'будет', 'плюсом', 'необходимо', 'от', 'до', 'руб', 'это',\n",
    "    'или', 'навык', 'владение'\n",
    "}\n",
    "\n",
    "def get_soup_with_retries(url, retries=3, delay=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Sec-Ch-Ua': '\"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "    }\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        if i < retries - 1:\n",
    "            time.sleep(delay * (i + 1))\n",
    "    return None\n",
    "\n",
    "def extract_job_links_hh(list_page_soup):\n",
    "    links = []\n",
    "    results_container = list_page_soup.find('div', {'data-qa': 'vacancy-serp__results'})\n",
    "    if not results_container:\n",
    "        results_container = list_page_soup.find('div', {'id': 'a11y-main-content'})\n",
    "        if not results_container:\n",
    "             return []\n",
    "\n",
    "    vacancy_cards = results_container.select('div[data-qa^=\"vacancy-serp__vacancy\"]')\n",
    "    if not vacancy_cards:\n",
    "         vacancy_cards = results_container.select('div:has(> h2 a[data-qa=\"serp-item__title\"])')\n",
    "\n",
    "    for card in vacancy_cards:\n",
    "        link_tag = card.select_one('a[data-qa=\"serp-item__title\"]')\n",
    "        if not link_tag:\n",
    "             link_tag = card.select_one('a.serp-item__title')\n",
    "\n",
    "        if link_tag and link_tag.get('href'):\n",
    "            href = link_tag['href'].split('?')[0]\n",
    "            if '/vacancy/' in href:\n",
    "                full_link = href if href.startswith('http') else BASE_URL_HH + href\n",
    "                if full_link not in links:\n",
    "                    links.append(full_link)\n",
    "    return links\n",
    "\n",
    "def extract_single_job_details_hh(job_page_soup, job_url):\n",
    "    title = \"N/A\"\n",
    "    title_tag = job_page_soup.find('h1', {'data-qa': 'vacancy-title'})\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "    salary_info = \"N/A\"\n",
    "    salary_tag = job_page_soup.find('div', {'data-qa': 'vacancy-salary'})\n",
    "    if salary_tag:\n",
    "         salary_text_tag = salary_tag.find('span', class_=lambda x: x and 'compensation-text' in x)\n",
    "         if salary_text_tag:\n",
    "             salary_info = salary_text_tag.get_text(separator=' ', strip=True).replace('\\u202f', ' ')\n",
    "         else:\n",
    "             salary_info = salary_tag.get_text(separator=' ', strip=True).replace('\\u202f', ' ')\n",
    "\n",
    "    company_name = \"N/A\"\n",
    "    company_tag = job_page_soup.find('a', {'data-qa': 'vacancy-company-name'})\n",
    "    if company_tag:\n",
    "        company_name = company_tag.get_text(strip=True)\n",
    "\n",
    "    location = \"N/A\"\n",
    "    location_span = job_page_soup.find('span', {'data-qa': 'vacancy-view-location'})\n",
    "    if location_span:\n",
    "        location = location_span.get_text(strip=True)\n",
    "    else:\n",
    "        address_p = job_page_soup.find('p', {'data-qa': 'vacancy-view-location'})\n",
    "        if address_p:\n",
    "             location = address_p.get_text(strip=True)\n",
    "        else:\n",
    "            address_div = job_page_soup.find('div', {'data-qa': 'vacancy-address'})\n",
    "            if address_div:\n",
    "                location = address_div.get_text(strip=True)\n",
    "\n",
    "    description = \"N/A\"\n",
    "    description_div = job_page_soup.find('div', {'data-qa': 'vacancy-description'})\n",
    "    if description_div:\n",
    "        description = description_div.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        desc_content_div = job_page_soup.find('div', class_=lambda x: x and 'vacancy-description' in x)\n",
    "        if desc_content_div:\n",
    "             desc_inner = desc_content_div.find('div', class_=lambda x: x and 'content' in x)\n",
    "             if desc_inner:\n",
    "                 description = desc_inner.get_text(separator='\\n', strip=True)\n",
    "             else:\n",
    "                 description = desc_content_div.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    skills_list = []\n",
    "    skills_section = job_page_soup.find('div', class_=lambda x: x and 'bloko-tag-list' in x)\n",
    "    if skills_section:\n",
    "        skill_tags = skills_section.find_all(lambda tag: tag.name in ['span', 'div'] and tag.has_attr('data-qa') and 'bloko-tag__text' in tag['data-qa'])\n",
    "        if not skill_tags:\n",
    "             skill_tags = skills_section.find_all(lambda tag: tag.name in ['span', 'div'] and 'bloko-tag__text' in tag.get('class', []))\n",
    "        skills_list = [skill.get_text(strip=True) for skill in skill_tags]\n",
    "\n",
    "    experience = \"N/A\"\n",
    "    exp_tag = job_page_soup.find('span', {'data-qa': 'vacancy-experience'})\n",
    "    if exp_tag:\n",
    "        experience = exp_tag.get_text(strip=True)\n",
    "\n",
    "    employment_mode = \"N/A\"\n",
    "    emp_tag = job_page_soup.find('p', {'data-qa': 'vacancy-view-employment-mode'})\n",
    "    if emp_tag:\n",
    "        employment_mode = emp_tag.get_text(strip=True)\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'salary_info': salary_info,\n",
    "        'company_name': company_name,\n",
    "        'location': location,\n",
    "        'experience': experience,\n",
    "        'employment_mode': employment_mode,\n",
    "        'skills_text': \", \".join(skills_list) if skills_list else \"N/A\",\n",
    "        'description': description,\n",
    "        'link': job_url\n",
    "    }\n",
    "\n",
    "def scrape_hh_jobs_data(search_query, area_id, num_postings_to_scrape=1000):\n",
    "    all_jobs_data = []\n",
    "    scraped_job_links = set()\n",
    "    page_num = 0\n",
    "    jobs_collected_this_run = 0\n",
    "    pbar = tqdm(total=num_postings_to_scrape, desc=\"Scraping jobs\", unit=\"job\")\n",
    "\n",
    "    while jobs_collected_this_run < num_postings_to_scrape and page_num < MAX_PAGES_TO_CHECK:\n",
    "        query_encoded = quote_plus(search_query)\n",
    "        current_url = f\"{BASE_URL_HH}/search/vacancy?text={query_encoded}&area={area_id}&page={page_num}\"\n",
    "\n",
    "        list_page_soup = get_soup_with_retries(current_url)\n",
    "        if not list_page_soup:\n",
    "            break\n",
    "\n",
    "        job_links_on_page = extract_job_links_hh(list_page_soup)\n",
    "\n",
    "        if not job_links_on_page:\n",
    "            if page_num == 0:\n",
    "                print(\"No job links found on first page. Check query/area/selectors.\")\n",
    "            break\n",
    "\n",
    "        new_links_found_on_this_page_count = 0\n",
    "        links_to_process_on_page = []\n",
    "        for job_link in job_links_on_page:\n",
    "            if job_link not in scraped_job_links:\n",
    "                 links_to_process_on_page.append(job_link)\n",
    "                 scraped_job_links.add(job_link)\n",
    "\n",
    "        if not links_to_process_on_page and page_num > 0:\n",
    "             break\n",
    "\n",
    "        for job_link in links_to_process_on_page:\n",
    "            if jobs_collected_this_run >= num_postings_to_scrape:\n",
    "                 break\n",
    "\n",
    "            new_links_found_on_this_page_count += 1\n",
    "            time.sleep(random.uniform(0.8, 1.8))\n",
    "\n",
    "            single_job_soup = get_soup_with_retries(job_link)\n",
    "            if single_job_soup:\n",
    "                try:\n",
    "                    job_details = extract_single_job_details_hh(single_job_soup, job_link)\n",
    "                    all_jobs_data.append(job_details)\n",
    "                    jobs_collected_this_run += 1\n",
    "                    pbar.update(1)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if jobs_collected_this_run >= num_postings_to_scrape:\n",
    "            break\n",
    "\n",
    "        page_num += 1\n",
    "        if jobs_collected_this_run < num_postings_to_scrape:\n",
    "            time.sleep(random.uniform(1.0, 2.5))\n",
    "\n",
    "    pbar.close()\n",
    "    return pd.DataFrame(all_jobs_data)\n",
    "\n",
    "def process_job_data_for_json(df):\n",
    "    if df.empty:\n",
    "        return {\"top_skills\": [], \"top_terms\": [], \"experience_levels\": [], \"salary_type_distribution\": []}\n",
    "\n",
    "    all_text_for_skills = []\n",
    "    if 'description' in df.columns:\n",
    "        all_text_for_skills.extend(df['description'].dropna().astype(str).tolist())\n",
    "    if 'skills_text' in df.columns:\n",
    "         all_text_for_skills.extend(df['skills_text'].dropna().astype(str).replace(',', ' ').tolist())\n",
    "\n",
    "    full_text_lower = ' '.join(all_text_for_skills).lower()\n",
    "    full_text_lower = re.sub(r'[^\\w\\s\\.\\+\\#-]', '', full_text_lower)\n",
    "\n",
    "    identified_skills_from_text = []\n",
    "    temp_text_for_multi_word = full_text_lower\n",
    "\n",
    "    multi_word_kws = sorted([kw for kw in SKILL_KEYWORDS_RU if any(c in kw for c in [' ', '.', '#', '+'])], key=len, reverse=True)\n",
    "    for skill_kw in multi_word_kws:\n",
    "        try:\n",
    "            if skill_kw == 'c++':\n",
    "                pattern = r'\\bc\\+\\+\\b'\n",
    "            elif skill_kw == 'c#':\n",
    "                pattern = r'\\bc#\\b'\n",
    "            else:\n",
    "                 pattern = r'\\b' + re.escape(skill_kw) + r'\\b'\n",
    "\n",
    "            count = len(re.findall(pattern, temp_text_for_multi_word, re.IGNORECASE))\n",
    "            if count > 0:\n",
    "                identified_skills_from_text.extend([skill_kw] * count)\n",
    "                temp_text_for_multi_word = re.sub(pattern, \"\", temp_text_for_multi_word, flags=re.IGNORECASE)\n",
    "        except re.error:\n",
    "             count = temp_text_for_multi_word.count(skill_kw)\n",
    "             if count > 0:\n",
    "                  identified_skills_from_text.extend([skill_kw] * count)\n",
    "                  temp_text_for_multi_word = temp_text_for_multi_word.replace(skill_kw, \"\")\n",
    "\n",
    "    single_word_tokens = word_tokenize(temp_text_for_multi_word)\n",
    "    filtered_single_tokens = [word for word in single_word_tokens if word not in STOP_WORDS_RU and len(word) > 1]\n",
    "    identified_skills_from_text.extend([word for word in filtered_single_tokens if word in SKILL_KEYWORDS_RU])\n",
    "    skill_counts = Counter(identified_skills_from_text)\n",
    "\n",
    "    descriptions_text = ' '.join(df['description'].dropna().astype(str).tolist()).lower()\n",
    "    general_tokens = word_tokenize(re.sub(r'[^\\w\\s]', '', descriptions_text))\n",
    "    general_filtered_tokens = [word for word in general_tokens if word not in STOP_WORDS_RU and len(word) > 2]\n",
    "    filtered_word_freq = Counter({word: count for word, count in Counter(general_filtered_tokens).items() if word not in COMMON_NOISE_RU})\n",
    "\n",
    "    experience_levels_list = []\n",
    "    if 'experience' in df.columns:\n",
    "        exp_counts = df['experience'].fillna('Не указан').value_counts()\n",
    "        experience_levels_list = [{\"name\": str(name), \"value\": int(value)} for name, value in exp_counts.items()]\n",
    "\n",
    "    salary_types_list = []\n",
    "    if 'salary_info' in df.columns:\n",
    "        def get_salary_type(s):\n",
    "            s_lower = str(s).lower()\n",
    "            if 'не указана' in s_lower or s == 'N/A':\n",
    "                 return 'Не указана'\n",
    "            elif 'от' in s_lower and 'до' in s_lower:\n",
    "                 return 'От/До (Диапазон)'\n",
    "            elif 'от' in s_lower:\n",
    "                 return 'От (Минимум)'\n",
    "            elif 'до' in s_lower:\n",
    "                 return 'До (Максимум)'\n",
    "            else:\n",
    "                 return 'Другое/Фиксированная'\n",
    "\n",
    "        df['salary_type_est'] = df['salary_info'].apply(get_salary_type)\n",
    "        salary_counts = df['salary_type_est'].value_counts()\n",
    "        salary_types_list = [{\"name\": str(name), \"value\": int(value)} for name, value in salary_counts.items()]\n",
    "\n",
    "    return {\n",
    "        \"top_skills\": [{\"name\": str(skill), \"value\": int(count)} for skill, count in skill_counts.most_common(20)],\n",
    "        \"top_terms\": [{\"name\": str(term), \"value\": int(count)} for term, count in filtered_word_freq.most_common(20)],\n",
    "        \"experience_levels\": experience_levels_list,\n",
    "        \"salary_type_distribution\": salary_types_list\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actual_num_to_scrape = min(NUM_JOBS_TO_SCRAPE, MAX_SCRAPE_LIMIT)\n",
    "    scraped_df = scrape_hh_jobs_data(TARGET_JOB_SEARCH_QUERY, TARGET_AREA_ID, num_postings_to_scrape=actual_num_to_scrape)\n",
    "\n",
    "    if not scraped_df.empty:\n",
    "        dashboard_data = process_job_data_for_json(scraped_df.copy())\n",
    "        output_json_filename = \"data.json\"\n",
    "        with open(output_json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nScraped {len(scraped_df)} job postings successfully.\")\n",
    "        print(f\"Dashboard data saved to {output_json_filename}\")\n",
    "        print(\"\\nSummary of processed data:\")\n",
    "        print(f\"  Top skills found: {len(dashboard_data.get('top_skills', []))}\")\n",
    "        print(f\"  Top terms found: {len(dashboard_data.get('top_terms', []))}\")\n",
    "        print(f\"  Experience level categories: {len(dashboard_data.get('experience_levels', []))}\")\n",
    "        print(f\"  Salary type categories: {len(dashboard_data.get('salary_type_distribution', []))}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No job postings were scraped.\")\n",
    "\n",
    "    print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceea833",
   "metadata": {},
   "source": [
    "### hh.ru has limitation on scraping, I tried 4 times with different approaches, result was always 787"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
